Привет! Я погрузился на три полных дня в задачу замены лица на одной картинке диффузионками, и вот результаты.

Вообще эту задачу можно долго крутить, поэтому ниже сначала покажу то, что успел, а потом объясню, как улучшил бы некоторые шаги.

*Егор Бурков, 2024-04-03*

![Image](https://github.com/user-attachments/assets/63f2de15-3c92-409a-ad37-c0f594b76579)

* [Метод на 3 (в этом репозитории) и инструкция, как запустить](#метод-на-3-в-этом-репозитории-и-инструкция-как-запустить)
* [Результаты](#результаты)
* [Метод на 4 (что я бы улучшил за дополнительные несколько дней)](#%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BD%D0%B0-4-%D1%87%D1%82%D0%BE-%D1%8F-%D0%B1%D1%8B-%D1%83%D0%BB%D1%83%D1%87%D1%88%D0%B8%D0%BB-%D0%B7%D0%B0-%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BD%D0%B5%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE-%D0%B4%D0%BD%D0%B5%D0%B9)
* [Метод на 4+ (как я сделал бы, будь больше видеопамяти и времени)](#%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BD%D0%B0-4-%D0%BA%D0%B0%D0%BA-%D1%8F-%D1%81%D0%B4%D0%B5%D0%BB%D0%B0%D0%BB-%D0%B1%D1%8B-%D0%B1%D1%83%D0%B4%D1%8C-%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B5-%D0%B2%D0%B8%D0%B4%D0%B5%D0%BE%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D0%B8-%D0%B8-%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%B8)
* [Метод на 5? (если бы была команда исследователей и много денег)](#%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BD%D0%B0-5-%D0%B5%D1%81%D0%BB%D0%B8-%D0%B5%D1%81%D1%82%D1%8C-%D0%BA%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D0%B0-%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9-%D0%B8-%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE-%D0%B4%D0%B5%D0%BD%D0%B5%D0%B3)

## Метод на 3 (в этом репозитории) и инструкция, как запустить

Это метод с дообучением "втупую". В следующем разделе объясню, как его можно улучшить.

1. Обрезаю фотки исходного человека вокруг лица - плюс-минус 100% места оставляю вокруг. Вдобавок еще и выровнял поворотом, но это не обязательно.

    В коде: `datasets/glam_ai_faces/train-rotated-cropped/`.
   
2. К фоткам исходного человека придумываю в целом верное, но незамысловатое описание. Я дописал ко всем "a photo of a woman face".

    В коде: `datasets/glam_ai_faces/train-rotated-cropped/metadata.csv`.

3. Обучаю на них LoRA к Stable Diffusion XL (сначала отладил на SD 1.5). В сгенерированных по тому же описанию картинках не обращаю внимания на "сюжет" (хотя с ним, очевидно, проблемы есть, о способах решения можно почитать ниже), а только на узнаваемость и качество. Выбираю чекпоинт, где уже узнается исходный человек, но все еще не вылезло пластилиновое лицо и морщинистые артефакты. Здесь выбрал 1800 итераций:

![Image](https://github.com/user-attachments/assets/6c656173-6863-40d1-9cd2-c7420ef09dc4)

    В коде: `train.sh` и `train_text_to_image_lora_sdxl.py`. Дообучение запускается через `bash train.sh`.

4. Рисую вручную к каждой **целевой** картинке маску лица:

![Image](https://github.com/user-attachments/assets/86677a5f-b9d4-45e5-a05e-d873f37314b2)

5. Придумываю описание к целевой картинке как на шаге 1. Тут не стал заморачиваться и оставил то же "a photo of a woman face".

6. Применяю стандартный diffusion inpainting: картинка обрезается вокруг лица примерно как в обучении (параметр `padding_mask_crop`), зашумляется на 30-70% (это регулирует параметр `strength`), потом на такую же степень денойзится обученной LoRA с использованием описания с шага 5, и наконец участок сгенерированной картинки, соответствующий маске, вставляется обратно в исходную картинку.

    `strength` подбираем максимальный, при котором нет сильных семантических сбоев и следов вставки. В моем случае оказалось, что на всех картинках это чаще всего 0.5.

    В коде: `inpaint.py`. Запускается через `python3 inpaint.py`.

## Результаты

![Image](https://github.com/user-attachments/assets/2ea935ed-232b-45dd-8bd4-c24ada3b31dd)

![Image](https://github.com/user-attachments/assets/cb547e36-7382-46f3-8b2b-fae45636abac)

![Image](https://github.com/user-attachments/assets/6a7b08ab-6d15-49e2-9a02-b35f006e6118)

![Image](https://github.com/user-attachments/assets/4c7e53bc-d8b6-4950-bc5a-8713155e0400)

![Image](https://github.com/user-attachments/assets/a260f2ed-a439-4d5f-94ab-80aceb65df70)

![Image](https://github.com/user-attachments/assets/cdcc1796-6ca8-4de3-8ea1-60f32a7abb7a)

![Image](https://github.com/user-attachments/assets/a57aed0f-8e49-4216-a336-1bdf953bcef2)

## Метод на 4 (что я бы улучшил за дополнительные несколько дней)

Результаты так себе, сомнений нет. Можно улучшить многое, в первую очередь узнаваемость исходного человека, да и просто автоматизировать кое-что. Вот несколько идей:

* **Шаг 1** - обрезку и поворот можно сделать автоматически детектором лиц.
*
* **Шаг 2** - дообучение будет лучше, если у фоток будет хорошее описание, которое затрагивает все, кроме самих черт лица. Его можно получить, задав несколько заранее придуманных вопросов какой-нибудь VQA-модели типа ViLT-VQA или BLIP-VQA: что происходит на фото? каково освещение и какого цвета? каково выражение лица человека? каков фон? как лежат волосы? и т.д.

* **Шаг 3** - можно взять другую модель. На реддите не раз читал уверенное мнение, что LoRA на лица нужно обучать на FLUX.1 и только: я попробовал ее, но сходу она не влезла по памяти, и поэтому забил. Еще можно было попробовать IP-Adapter - вижу, что с лицами его тоже часто используют.

* **Шаг 3** - можно увеличить датасет, чтоб поменьше переобучаться. Все-таки 7 фото - это очень мало. Это можно сделать разными способами, от сбора новых фото до генерации синтетики вспомогательными методами (можно хоть самыми тупыми, типа зафиттить template mesh и отрисовать его с разных сторон; можно продвинутыми методами создания аватаров по одной картинке).

* **Шаг 4** - это можно сделать, предсказав ключевые точки лица готовым алгоритмом (мой любимый когда-то был [этот](https://github.com/1adrianb/face-alignment)), закрасив область между ними и немного размыв ее.

* **Шаг 5** - тот же комментарий, что и к шагу 2. Тут это может быть еще важнее, ведь вдруг мы захотим не просто заменить лицо, а сохранить выражение лица.

* **Шаг 6** - если времени и ресурсов куча (с трудом представляю такой сценарий, но вдруг), то strength можно подбирать онлайн по какой-нибудь важной нам метрике (расстояние между дескрипторами insightface, метрики реализма/привлекательности..).

## Метод на 4+ (как я сделал бы, будь больше видеопамяти и времени)

Недавно вышла прикольная статья [InfiniteYou от ByteDance](https://huggingface.co/ByteDance/InfiniteYou) с открытой моделью. Это FLUX.1, который обучили с дополнительным входом (аналогично ControlNet, но другая архитектура) - вектор identity из InsightFace. Их модель сразу генерирует какого надо человека.

![](https://huggingface.co/ByteDance/InfiniteYou/resolve/main/assets/teaser.jpg)

Я бы сделал на ней "offline"- (one-shot-) подход:

1. Вычисляем дескриптор InsightFace нашего человека.
2. Подаем его в InfiniteYou InfuseNet и делаем inpainting целевой фотки, т.е. повторяем шаги 4-6 из моего метода.
3. Если узнаваемость недостаточная, то обучаем LoRA (шаги 1-3 из моего метода) и пробуем другой вариант их модели: там есть один вариант для большей красоты и один для лучшей узнаваемости.

Я пробовал это сделать и даже написал inpainting-интерфейс (пока модель грузилась блин), но быстро уперся в нехватку видеопамяти. Базовые оптимизации типа group offloading не помогли. Нужно было либо найти емкую видеокарту, либо потратить время на оптимизацию, поэтому я забил.

## Метод на 5? (если есть команда исследователей и много денег)

Даже если тот метод будет работать идеально, к нему много вопросов. А он будет консистентно работать на кадрах видео? А если надо очень быстро генерировать? и т.д.

Например, если нужна будет low-latency генерация или обработка на телефоне, то не обойдется без GANов + придется копнуть в сторону быстрой диффузии. Статей на эту тему выходит много, но пока есть куда расти, и готовой качественной модели нету, придется допиливать.
